{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54edff3a",
   "metadata": {},
   "source": [
    "Pytorch Training for EMNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b498d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11a9c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader   \n",
    "# See https://clay-atlas.com/us/blog/2021/08/25/pytorch-en-random-split-data-set/\n",
    "from torchvision import datasets       # Gets standard data sets from internet\n",
    "from torchvision.transforms import ToTensor, Lambda \n",
    "\n",
    "# ToTensor() converts numpy array to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430db422",
   "metadata": {},
   "source": [
    "# Adjustable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23aa080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "batch_size=40 # Minibatch Size\n",
    "learning_rate = 0.00005 # Learning Rate\n",
    "epochs = 60 # Epochs\n",
    "\n",
    "\n",
    "# Do you want to run this on all available gpu's? True for yes, False for no\n",
    "gpu_run = True\n",
    "# Run on multiple GPU's\n",
    "parallelize = False\n",
    "\n",
    "# Which network do you want to use? 'stable' or 'experimental'\n",
    "which__net = 'experimental'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbb81d",
   "metadata": {},
   "source": [
    "# Import Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11413b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EMNIST Data as the training data\n",
    "training_data = datasets.EMNIST(root = \"./data\", # Where are we putting it? \n",
    "                                                split = 'balanced' , # decide on the dataset split you want \n",
    "                                                train = True, # is this the training data?\n",
    "                                                download = True, # Do you want to download the data to the local machine\n",
    "                                                transform = ToTensor(), # Transform the outcoming data\n",
    "                                                )\n",
    "\n",
    "# Length of testing and validation data splits \n",
    "vdat = int(len(training_data)/6) # make 1/6th of the testing data validation\n",
    "tdat = int(len(training_data) - vdat) # make the rest testing\n",
    "\n",
    "# Split the training and validation data using random_split\n",
    "training_data, validation_data = random_split(training_data,[tdat, vdat],\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Import Testing Data\n",
    "test_data = datasets.EMNIST(root = \"./data\", # Where are we putting it? \n",
    "                                                split = 'balanced' , # decide on the dataset split you want \n",
    "                                                train = False, # is this the training data?\n",
    "                                                download = True, # Do you want to download the data to the local machine\n",
    "                                                transform = ToTensor(), # Transform the outcoming data\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f92b5",
   "metadata": {},
   "source": [
    "# Shape and Length of the datasets\n",
    "\n",
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "471d63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(datasets) = <class 'module'>\n",
      "type(training_data) = <class 'torch.utils.data.dataset.Subset'>\n",
      "len(training_data) = 94000\n",
      "type(training_data[0]) = <class 'tuple'>\n",
      "type(training_data[1]) = <class 'tuple'>\n",
      "len(training_data[0]) = 2\n",
      "training_data[0][0].shape =  torch.Size([1, 28, 28])\n",
      "training_data[0][1] = 40\n",
      "img.shape = torch.Size([1, 28, 28])\n",
      "label = 40\n"
     ]
    }
   ],
   "source": [
    "print('type(datasets) =', type(datasets))\n",
    "print('type(training_data) =', type(training_data))\n",
    "print('len(training_data) =', len(training_data))\n",
    "print('type(training_data[0]) =', type(training_data[0]))\n",
    "print('type(training_data[1]) =', type(training_data[1]))\n",
    "print('len(training_data[0]) =', len(training_data[0]))\n",
    "print('training_data[0][0].shape = ', training_data[0][0].shape)\n",
    "print('training_data[0][1] =', training_data[0][1])\n",
    "img,label = training_data[0]\n",
    "print('img.shape =', img.shape)\n",
    "print('label =',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd03a5",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7de9ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(datasets) = <class 'module'>\n",
      "type(validation_data) = <class 'torch.utils.data.dataset.Subset'>\n",
      "len(validation_data) = 18800\n",
      "type(validation_data[0]) = <class 'tuple'>\n",
      "type(validation_data[1]) = <class 'tuple'>\n",
      "len(validation_data[0]) = 2\n",
      "validation_data[0][0].shape =  torch.Size([1, 28, 28])\n",
      "validation_data[0][1] = 26\n",
      "img.shape = torch.Size([1, 28, 28])\n",
      "label = 26\n"
     ]
    }
   ],
   "source": [
    "print('type(datasets) =', type(datasets))\n",
    "print('type(validation_data) =', type(validation_data))\n",
    "print('len(validation_data) =', len(validation_data))\n",
    "print('type(validation_data[0]) =', type(validation_data[0]))\n",
    "print('type(validation_data[1]) =', type(validation_data[1]))\n",
    "print('len(validation_data[0]) =', len(validation_data[0]))\n",
    "print('validation_data[0][0].shape = ', validation_data[0][0].shape)\n",
    "print('validation_data[0][1] =', validation_data[0][1])\n",
    "img,label = validation_data[0]\n",
    "print('img.shape =', img.shape)\n",
    "print('label =',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913450d",
   "metadata": {},
   "source": [
    "# Create Minibatches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56e89d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data minibatches\n",
    "train_dataloader = DataLoader(training_data, # what data are we loading?\n",
    "                                                batch_size=batch_size,  # what is the minibatch size? \n",
    "                                                shuffle=True) # shuffle to randomize\n",
    "\n",
    "# Testing data minibatches\n",
    "test_dataloader = DataLoader(test_data, # what data are we loading?\n",
    "                                                batch_size=batch_size, # what is the minibatch size going to be\n",
    "                                                shuffle=True) # shuffle to randomize the order\n",
    "\n",
    "# Validation dataset Loader                                                \n",
    "validation_dataloader = DataLoader(validation_data,  # what dataset are we loading? \n",
    "                                                        batch_size=batch_size, # what is the batch size going to be?\n",
    "                                                        shuffle=True) # Shuffle to randomize the order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed361",
   "metadata": {},
   "source": [
    "## Testing Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a532bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "len(test_dataloader) = 470\n",
      "len(test_dataloader.dataset) = 18800\n",
      "len(test_dataloader)*batch_size = 18800\n",
      "\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "len(train_dataloader) = 2350\n",
      "len(train_dataloader.dataset) = 94000\n",
      "len(train_data_loader)*batch_size = 94000\n",
      "\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "len(validation_dataloader) = 470\n",
      "len(validation_dataloader.dataset) = 18800\n",
      "len(validation_dataloader)*batch_size = 18800\n"
     ]
    }
   ],
   "source": [
    "print(type(test_dataloader))\n",
    "print('len(test_dataloader) =',len(test_dataloader))   # number of batches\n",
    "print('len(test_dataloader.dataset) =', len(test_dataloader.dataset))\n",
    "print('len(test_dataloader)*batch_size =',len(test_dataloader)*batch_size)\n",
    "print()\n",
    "print(type(train_dataloader))\n",
    "print('len(train_dataloader) =',len(train_dataloader))   # number of batches\n",
    "print('len(train_dataloader.dataset) =', len(train_dataloader.dataset))\n",
    "print('len(train_data_loader)*batch_size =',len(train_dataloader)*batch_size) \n",
    "print()\n",
    "print(type(validation_dataloader))\n",
    "print('len(validation_dataloader) =',len(validation_dataloader))   # number of batches\n",
    "print('len(validation_dataloader.dataset) =', len(validation_dataloader.dataset)) \n",
    "print('len(validation_dataloader)*batch_size =',len(validation_dataloader)*batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338b403",
   "metadata": {},
   "source": [
    "## Example Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e9c0a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device \n"
     ]
    }
   ],
   "source": [
    "# # train_dataloader is not subscriptable so train_dataloader[0] gives an error\n",
    "\n",
    "# batch_example = iter(train_dataloader).next()\n",
    "# print('len(batch_example) =',len(batch_example))\n",
    "# print('type(batch_example) =',type(batch_example))\n",
    "# print()\n",
    "\n",
    "# batch_images = batch_example[0]\n",
    "# print('len(batch_images) =',len(batch_images))\n",
    "# print('type(batch_images) =', type(batch_images))\n",
    "\n",
    "# batch_labels = batch_example[1]\n",
    "# print()\n",
    "# print('batch_labels =', batch_labels)\n",
    "# print('type(batch_labels) =', type(batch_labels))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device '.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276c8d",
   "metadata": {},
   "source": [
    "# Design the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82f9b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using experimental network\n"
     ]
    }
   ],
   "source": [
    "# from tkinter.ttk import _Padding\n",
    "import torch.nn.functional as F\n",
    "if which__net == 'stable':\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, stride = 1, kernel_size = 5)  # makes 20 maps of 24x24\n",
    "            self.pool = nn.MaxPool2d(2, 2)                              # 20 maps of 12x12\n",
    "            self.conv2 = nn.Conv2d(20, 40, stride = 1, kernel_size = 5) # 40 maps of 8x8\n",
    "            self.pool = nn.MaxPool2d(2, 2)                              # 40 maps of 4x4\n",
    "            self.fc1 = nn.Linear(40 * 4 * 4, 100)                       # flatten to 40*4*4 neurons                   \n",
    "                    \n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = F.relu(self.fc1(x))\n",
    "            return x\n",
    "#############################################################################\n",
    "\n",
    "if which__net == 'experimental':\n",
    "    # ############# New net##################\n",
    "     # ############# New net##################\n",
    "    print('Using experimental network')\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # makes 20 maps of 28x28\n",
    "            self.conv1 = nn.Conv2d(1, 20, # Channels in/Out \n",
    "                                                    stride = 1, # Stride\n",
    "                                                    kernel_size = 3,\n",
    "                                                    padding = 1)  # Kernel\n",
    "\n",
    "            # Keep the size, add 10 more layers\n",
    "            self.conv2 = nn.Conv2d(20, 30, # Channels in/Out \n",
    "                                                    stride = 1, # Stride\n",
    "                                                    kernel_size = 3,\n",
    "                                                    padding = 1)  # Kernel\n",
    "\n",
    "            # Downsampled and thus altering parameters\n",
    "            self.conv3 = nn.Conv2d(30  , 30 ,\n",
    "                                                     stride = 1,\n",
    "                                                     kernel_size = 3,\n",
    "                                                     padding = 1) \n",
    "\n",
    "            # Keeping size and reducing layers\n",
    "            self.conv4 = nn.Conv2d(30  , 10 ,\n",
    "                                                     stride = 1,\n",
    "                                                     kernel_size = 3,\n",
    "                                                     padding = 1) \n",
    "            # Pooling function to divide size in hafl\n",
    "            self.pool = nn.MaxPool2d(2, 2)   # 40 maps of 4x4\n",
    "            \n",
    "            # fully connected layer structure\n",
    "            self.fc1 = nn.Linear(10 * 7 * 7, 300)                          \n",
    "            self.fc2 = nn.Linear(300 , 160)                     # Added\n",
    "            self.fc3 = nn.Linear(160, 120)                       # Added    \n",
    "\n",
    "        def forward(self, x):\n",
    "            # Convolutions Period\n",
    "            # First Round\n",
    "            x = F.rrelu(self.conv1(x))\n",
    "            x = F.rrelu(self.conv2(x))\n",
    "            x = self.pool(x)\n",
    "\n",
    "            # Second Round\n",
    "            x = F.rrelu(self.conv3(x))\n",
    "            x = F.rrelu(self.conv4(x))\n",
    "            x = self.pool(x)\n",
    "            # x = self.pool(F.rrelu(self.conv3(x))) # Added\n",
    "\n",
    "            # Flatten everything in order to pass through FC Layers\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "            # Linear Layers\n",
    "            x = F.rrelu(self.fc1(x))\n",
    "            x = F.rrelu(self.fc2(x)) # Added \n",
    "            x = F.rrelu(self.fc3(x)) # Added\n",
    "            # x = F.softmax(self.fc4(x), dim =1) # Added\n",
    "                # Decided against softmax based on Ian/Goodfellow chapter 6 https://stats.stackexchange.com/questions/218752/relu-vs-sigmoid-vs-softmax-as-hidden-layer-neurons\n",
    "            # x = self.dropout(x) # Added \n",
    "                ## Removed because it is just elminating the random variables\n",
    "            return x\n",
    "    ##########################################\n",
    "\n",
    "    model = NeuralNetwork().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# Mods\n",
    "# strides set from 1 to 2  :/\n",
    "# https://paperswithcode.com/method/u-net\n",
    "\n",
    "# some modifications pulled from \n",
    "# https://github.com/austin-hill/EMNIST-CNN/blob/master/torch_cnn.py \n",
    "# Added Dropout\n",
    "\n",
    "\n",
    "# Switched \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891430f",
   "metadata": {},
   "source": [
    "# Run on GPU And parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "949b1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(30, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=490, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=160, bias=True)\n",
      "  (fc3): Linear(in_features=160, out_features=120, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# if gpu_run == True:\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     print('Using {} device '.format(device))\n",
    "#     model =  NeuralNetwork().to(device)\n",
    "\n",
    "#     #########################PARALLEL GPU'S#########################################\n",
    "#     # This was pulled from the parallelization tutorial in pytorch\n",
    "#     # https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\n",
    "#     if parallelize == True:\n",
    "#         if torch.cuda.device_count() > 1:\n",
    "#             print(\"Using\", torch.cuda.device_count(), \"GPU's\")\n",
    "#             model = nn.DataParallel(model)  \n",
    "\n",
    "#         # Transfer the model to the chosen platform \n",
    "#         model.to(device)\n",
    "#         # model =  NeuralNetwork().to(device)\n",
    "#         print(model)\n",
    "\n",
    "# else:\n",
    "#     model =  Net().to(device)\n",
    "#     print(model)\n",
    "#     print('Running on CPU')\n",
    "\n",
    "\n",
    "model =  NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aef92b",
   "metadata": {},
   "source": [
    "# Define the training loop process\n",
    "\n",
    "This takes the data loader, model, loss function, and optimization function as input and, for the length of the dataset, pushes the input and output to whatever device is being used to then predict the model for X and look at the loss function for the predicted vs. y. \n",
    "\n",
    "This information is then used for backwards propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9143d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X = X.to(device) \n",
    "        y = y.to(device)\n",
    "        pred = model(X) # Run X  through the model\n",
    "        loss = loss_fn(pred,y) # Compare the output to y through the loss function\n",
    "    \n",
    "        optimizer.zero_grad() # Zero the gradient\n",
    "        loss.backward() # Backwards propogation\n",
    "        optimizer.step() # Re-evaluate the model and return the loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c900c4",
   "metadata": {},
   "source": [
    "# Define the testing loop process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae5b84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_correct = []\n",
    "def test_loop(dataloader, model, loss_fn, valid_or_test_flag = \"valid\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item() # apply loss function\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct = int(correct)\n",
    "    accuracy = correct/size\n",
    "    if valid_or_test_flag == \"test\":\n",
    "        print(f\"Test Error: \\n Accuracy {(100*correct):>0.1f}%, Avg. loss: {test_loss:>8f} \\n\")\n",
    "        # print(f\"Accuracy on Test Data: \\n {correct}/{size} or {(100*accuracy):>0.1f} percent% \\n\")\n",
    "        # print(f\"Average Test Loss: \\n {test_loss:>8f} \\n\")\n",
    "    else: \n",
    "        print(f\"Accuracy on Validation Data: \\n {correct}/{size} or {(100*accuracy):>0.1f} percent% \\n\")\n",
    "        print(f\"Average Validation Loss: \\n {test_loss:>8f} \\n\")\n",
    "        validation_correct.append(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e512fe",
   "metadata": {},
   "source": [
    "# Define loss functino, optimization, and run through each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84a5416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 12111/18800 or 64.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.030799 \n",
      "\n",
      "Epoch 2)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 13370/18800 or 71.1 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.024586 \n",
      "\n",
      "Epoch 3)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 13773/18800 or 73.3 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.022098 \n",
      "\n",
      "Epoch 4)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 14212/18800 or 75.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.020014 \n",
      "\n",
      "Epoch 5)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 14473/18800 or 77.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.018373 \n",
      "\n",
      "Epoch 6)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 14813/18800 or 78.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.016977 \n",
      "\n",
      "Epoch 7)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15002/18800 or 79.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.016086 \n",
      "\n",
      "Epoch 8)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15142/18800 or 80.5 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.015254 \n",
      "\n",
      "Epoch 9)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15287/18800 or 81.3 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.014309 \n",
      "\n",
      "Epoch 10)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15525/18800 or 82.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.013568 \n",
      "\n",
      "Epoch 11)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15533/18800 or 82.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.013081 \n",
      "\n",
      "Epoch 12)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15680/18800 or 83.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.012289 \n",
      "\n",
      "Epoch 13)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15824/18800 or 84.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011916 \n",
      "\n",
      "Epoch 14)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15902/18800 or 84.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011477 \n",
      "\n",
      "Epoch 15)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15986/18800 or 85.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011156 \n",
      "\n",
      "Epoch 16)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 15986/18800 or 85.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010982 \n",
      "\n",
      "Epoch 17)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16113/18800 or 85.7 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010633 \n",
      "\n",
      "Epoch 18)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16084/18800 or 85.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010525 \n",
      "\n",
      "Epoch 19)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16151/18800 or 85.9 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010275 \n",
      "\n",
      "Epoch 20)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16185/18800 or 86.1 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010173 \n",
      "\n",
      "Epoch 21)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16203/18800 or 86.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010090 \n",
      "\n",
      "Epoch 22)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16232/18800 or 86.3 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009908 \n",
      "\n",
      "Epoch 23)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16281/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009953 \n",
      "\n",
      "Epoch 24)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16175/18800 or 86.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010408 \n",
      "\n",
      "Epoch 25)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16256/18800 or 86.5 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009935 \n",
      "\n",
      "Epoch 26)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16206/18800 or 86.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010241 \n",
      "\n",
      "Epoch 27)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16305/18800 or 86.7 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009751 \n",
      "\n",
      "Epoch 28)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16362/18800 or 87.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009746 \n",
      "\n",
      "Epoch 29)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16326/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009726 \n",
      "\n",
      "Epoch 30)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16358/18800 or 87.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009734 \n",
      "\n",
      "Epoch 31)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16260/18800 or 86.5 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010134 \n",
      "\n",
      "Epoch 32)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16356/18800 or 87.0 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009658 \n",
      "\n",
      "Epoch 33)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16336/18800 or 86.9 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009821 \n",
      "\n",
      "Epoch 34)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16313/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009953 \n",
      "\n",
      "Epoch 35)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16325/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.009805 \n",
      "\n",
      "Epoch 36)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16317/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010039 \n",
      "\n",
      "Epoch 37)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16325/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010168 \n",
      "\n",
      "Epoch 38)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16319/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010031 \n",
      "\n",
      "Epoch 39)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16312/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010180 \n",
      "\n",
      "Epoch 40)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16382/18800 or 87.1 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010207 \n",
      "\n",
      "Epoch 41)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16221/18800 or 86.3 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010543 \n",
      "\n",
      "Epoch 42)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16309/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010475 \n",
      "\n",
      "Epoch 43)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16278/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010491 \n",
      "\n",
      "Epoch 44)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16279/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010624 \n",
      "\n",
      "Epoch 45)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16318/18800 or 86.8 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010615 \n",
      "\n",
      "Epoch 46)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16247/18800 or 86.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010841 \n",
      "\n",
      "Epoch 47)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16296/18800 or 86.7 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011064 \n",
      "\n",
      "Epoch 48)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16212/18800 or 86.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011199 \n",
      "\n",
      "Epoch 49)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16245/18800 or 86.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.010988 \n",
      "\n",
      "Epoch 50)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16252/18800 or 86.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011331 \n",
      "\n",
      "Epoch 51)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16276/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011170 \n",
      "\n",
      "Epoch 52)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16282/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011501 \n",
      "\n",
      "Epoch 53)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16253/18800 or 86.5 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011664 \n",
      "\n",
      "Epoch 54)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16252/18800 or 86.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011621 \n",
      "\n",
      "Epoch 55)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16275/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011693 \n",
      "\n",
      "Epoch 56)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16276/18800 or 86.6 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.011815 \n",
      "\n",
      "Epoch 57)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16240/18800 or 86.4 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.012031 \n",
      "\n",
      "Epoch 58)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16201/18800 or 86.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.012399 \n",
      "\n",
      "Epoch 59)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16203/18800 or 86.2 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.012485 \n",
      "\n",
      "Epoch 60)\n",
      " --------------------------\n",
      "Accuracy on Validation Data: \n",
      " 16228/18800 or 86.3 percent% \n",
      "\n",
      "Average Validation Loss: \n",
      " 0.012494 \n",
      "\n",
      "Done Training\n",
      "\n",
      "Begin Test\n",
      "Test Error: \n",
      " Accuracy 1608000.0%, Avg. loss: 0.013414 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizaiton function\n",
    "# optimizer = torch.optim.SGD(model.parameters(),\n",
    "#                                                     lr = learning_rate)\n",
    "                                                    \n",
    "# Optimizaiton function\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                                    lr = learning_rate)\n",
    "# for each epoch\n",
    "for t in range(epochs):\n",
    "    # Print what epoch we are on\n",
    "    print(f\"Epoch {t+1})\\n --------------------------\")\n",
    "    # Run the training loop with:\n",
    "    train_loop(train_dataloader,  # data\n",
    "                        model, # model we are using\n",
    "                        loss_fn, # loss function\n",
    "                        optimizer) # appropriate optimizer\n",
    "\n",
    "    # Run the testing loop with:\n",
    "    test_loop(validation_dataloader, # The validation dataset \n",
    "                        model, # the model specified\n",
    "                        loss_fn, # The loss function\n",
    "                        valid_or_test_flag = \"valid\") # Print the validation rate\n",
    "\n",
    "# After we hav run through all the epochs\n",
    "print(\"Done Training\")\n",
    "print()\n",
    "print(\"Begin Test\")\n",
    "\n",
    "# Run the model on the testing data\n",
    "test_loop(test_dataloader,  # input the testing data\n",
    "                    model,  # Using the trained model\n",
    "                    loss_fn, # Using the specified loss function\n",
    "                    valid_or_test_flag = \"test\") # Look at the testing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6a99f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c238f6d",
   "metadata": {},
   "source": [
    "# Save the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './EMNIST_net.pth' # Path that we are going to save to\n",
    "torch.save(model.state_dict(), PATH) # Save the network to this given path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5701d3b",
   "metadata": {},
   "source": [
    "### Save the plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = 'EMNIST_plot_data.json'\n",
    "f = open(filename, \"w\")\n",
    "json.dump(validation_correct, f)\n",
    "f.close() \n",
    "\n",
    "validation_correct_xmin = 0\n",
    "validation_cost_xmin = 0\n",
    "validation_set_size = len(validation_data)\n",
    "\n",
    "filename = 'EMNIST_plot_parameters.json'\n",
    "f = open(filename, \"w\")\n",
    "json.dump([epochs, validation_correct_xmin, validation_set_size], f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11f201",
   "metadata": {},
   "source": [
    "## Assess the overall accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e315c7f86a17d253a6d072d6de69d65ab47ea1db8cf163bddb69875676639286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
